{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nesse notebook, vou testar com BoW. Também irei tester alguns novos aproaches.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Import Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustavo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import operator \n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o arquivo de treino é muito grando, vamos ter que limitar a quantidade de linhas utilizadas, pois só tenho 4gb ram. Também será necessário utilizar o encoder UTF-8, pois os textos são em português e espanhol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1999645, 2)\n",
      "Test shape :  (246955, 2)\n",
      "00:00:13\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "p = 0.1\n",
    "train = pd.read_csv('train.csv', header = 0, encoding = 'utf-8', skiprows = lambda i: i>0 and random.random() >p)\n",
    "test = pd.read_csv('test.csv', header = 0, encoding = 'utf-8')\n",
    "\n",
    "train.drop(['language','label_quality'], axis = 'columns', inplace = True) \n",
    "test.drop(['language'], axis = 'columns', inplace = True) \n",
    "\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)\n",
    "\n",
    "elapsed_time = int(time.time() - start_time)\n",
    "print('{:02d}:{:02d}:{:02d}'.format(elapsed_time // 3600, (elapsed_time % 3600 // 60), elapsed_time % 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Kit Maternidade Bolsa-mala Baby/bebe Vinho Men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Trocador De Fraldas Fisher Price Feminino Rosa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Motor Ventoinha - Fiat Idea / Palio 1.8 - A 04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Amortecedor Mola Batente D Dir New Civic 14 - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Cadeirinha De Carro Bebê Princesa Princess 9 A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title\n",
       "0   0  Kit Maternidade Bolsa-mala Baby/bebe Vinho Men...\n",
       "1   1  Trocador De Fraldas Fisher Price Feminino Rosa...\n",
       "2   2  Motor Ventoinha - Fiat Idea / Palio 1.8 - A 04...\n",
       "3   3  Amortecedor Mola Batente D Dir New Civic 14 - ...\n",
       "4   4  Cadeirinha De Carro Bebê Princesa Princess 9 A..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   0\n",
       "1   1\n",
       "2   2\n",
       "3   3\n",
       "4   4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorias = train.filter(['category'], axis = 1)\n",
    "num = test.filter(['id'], axis = 1)\n",
    "num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como nas competições é ideal juntar os datasets, iremos fazer isso agora:\n",
    "train.drop(['category'], axis = 'columns', inplace = True) \n",
    "test.drop(['id'], axis = 'columns', inplace = True) \n",
    "\n",
    "df = pd.concat([train, test], axis=0)\n",
    "del(train, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246600, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Preprocessing Text\n",
    "\n",
    "**Aqui iremos criar as embbedings, palavras chaves para o nosso modelo treinar. É uma espécie de dicionário que deve conter pelo menos 95% do texto.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algumas bibliotecas que temos que importar para poder realizar essa parte\n",
    "# esses que estão desmarcados já foram importados.\n",
    "#import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "#import operator\n",
    "#import re\n",
    "#from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse próximo passo iremos criar nosso vocabulário de teste. Essa função irá percorrer todo o texto e contar quantas ocorrências de cada palavra temos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos montar o dicionário. A função *pregress_apply* permite que acompanhemos o tempo decorrido sobre um rotina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 2246600/2246600 [00:04<00:00, 479579.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 2246600/2246600 [00:04<00:00, 484492.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hidrolavadora': 1517, 'Lavor': 51, 'One': 5525, '120': 4692, 'Bar': 4316}\n"
     ]
    }
   ],
   "source": [
    "sentences = df[\"title\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui é a hora de carregar o modelo já treinado de dados, no caso do FastText\n",
    "#import fastText\n",
    "#pd.read_csv(\"../data_folder/data.csv\")\n",
    "embeddings_index = KeyedVectors.load_word2vec_format('cbow_s1000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/dccuchile/spanish-word-embeddings\n",
    "#trained_words = 'crawl-300d-2M.vec'\n",
    "embeddings_index2 = KeyedVectors.load_word2vec_format('SBW-vectors-300-min5.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, com essa função, vamos checar a intersecção entre nosso vocabulário e as embeddings. Ela vai gerar uma lista oov(output  out of vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab,embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    sorted_x = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 909210/909210 [00:04<00:00, 198879.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 0.220% of vocab\n",
      "Found embeddings for  3.302% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 2246600/2246600 [00:15<00:00, 146197.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 2246600/2246600 [00:04<00:00, 454390.36it/s]\n"
     ]
    }
   ],
   "source": [
    "df[\"title\"] = df[\"title\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = df[\"title\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 650637/650637 [00:04<00:00, 154942.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 3.339% of vocab\n",
      "Found embeddings for  1.635% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'] = df['title'].apply(lambda x: x.lower())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 1 character terms (length == 1)\n",
    "df['title'] = df['title'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 2246600/2246600 [00:09<00:00, 230988.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 106/106 [00:00<00:00, 51386.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 83.962% of vocab\n",
      "Found embeddings for  81.145% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df['title'])\n",
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_1 = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\*)|(\\d+)|(\\+)|(\\%)|(\\&)|(\\/)|(\\-)\")\n",
    "                       \n",
    "def clean_reviews(reviews):\n",
    "    reviews = [replace_1.sub(\"\", line.lower()) for line in reviews]\n",
    "    return reviews\n",
    "                       \n",
    "df['title'] = clean_reviews(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 2246600/2246600 [00:08<00:00, 256929.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 96/96 [00:00<00:00, 98762.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 91.667% of vocab\n",
      "Found embeddings for  85.238% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df['title'])\n",
    "oov = check_coverage(vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1247331/1247331 [00:02<00:00, 578600.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1247331/1247331 [00:03<00:00, 319786.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1247331/1247331 [00:02<00:00, 591471.42it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = df[\"title\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['à',' ','\\xa0','\\x9d','\\x81','\\x7f','\\x8d','\\x90','°','\\x9d','´','¡','®','¿','¨','×','»','·','¦','«','±','§','¢','£','\\xad','\\x81']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'pçs':'peças',\n",
    "                'soporte':'suporte',\n",
    "                'delantero':'dianteiro',\n",
    "                'cargador':'carregador',\n",
    "                'lampara':'lâmpada',\n",
    "                'talle':'tamanho',\n",
    "                'cuotas':'dívidas',\n",
    "                'embrague':'embreagem',\n",
    "                'plegable':'dobrável',\n",
    "                'inoxidable':'inoxidável',\n",
    "                'impecable':'impecável',\n",
    "                'accesorios':'acessórios',\n",
    "                'inflable':'inflável',\n",
    "                'estuche':'kit',\n",
    "                'griferia':'torneira',\n",
    "                'heladera': 'refrigerador',\n",
    "                'compresor': 'compressor',\n",
    "                'cubre': 'cobre',\n",
    "                'silicona': 'silicone',\n",
    "                'impresora': 'impressora'\n",
    "\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title\"] = df[\"title\"].apply(lambda x: replace_typical_misspell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = check_coverage(vocab, embeddings_index2)\n",
    "oov[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer uma limpeza maior e também processar um embedding do dicionário em espanhol. Mas por enquanto vamos ver como estamos fazendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999645, 1)\n",
      "(246955, 1)\n"
     ]
    }
   ],
   "source": [
    "#Primeiro vamos fazer a divisão do treino e teste de volta\n",
    "train = df.iloc[:1999645,:]\n",
    "test = df.iloc[1999645:,:]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hidrolavadora lavor one  bar w bomba aluminio ...</td>\n",
       "      <td>ELECTRIC_PRESSURE_WASHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>painel para tv  polegadas quirino branco canela</td>\n",
       "      <td>TV_STORAGE_UNITS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ers oficial nacional baloncesto asociación inc...</td>\n",
       "      <td>LED_STAGE_LIGHTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carenagem tampa lateral nxr bros  vermelho</td>\n",
       "      <td>MOTORCYCLE_CLUTCH_COVERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>carregador bateria original câmera sony bc trv...</td>\n",
       "      <td>CAMERA_CHARGERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  hidrolavadora lavor one  bar w bomba aluminio ...   \n",
       "1    painel para tv  polegadas quirino branco canela   \n",
       "2  ers oficial nacional baloncesto asociación inc...   \n",
       "3        carenagem tampa lateral nxr bros  vermelho    \n",
       "4  carregador bateria original câmera sony bc trv...   \n",
       "\n",
       "                    category  \n",
       "0  ELECTRIC_PRESSURE_WASHERS  \n",
       "1           TV_STORAGE_UNITS  \n",
       "2           LED_STAGE_LIGHTS  \n",
       "3   MOTORCYCLE_CLUTCH_COVERS  \n",
       "4            CAMERA_CHARGERS  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Agora vamos juntar novamente no treino a coluna de categoria e o id para teste\n",
    "treino = pd.concat([train, categorias], axis=1)\n",
    "treino.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>kit maternidade bolsa mala baby bebe vinho men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>trocador de fraldas fisher price feminino rosa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>motor ventoinha fiat idea palio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>amortecedor mola batente dir new civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>cadeirinha de carro bebê princesa princess  kgs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title\n",
       "0   0  kit maternidade bolsa mala baby bebe vinho men...\n",
       "1   1  trocador de fraldas fisher price feminino rosa...\n",
       "2   2                  motor ventoinha fiat idea palio  \n",
       "3   3           amortecedor mola batente dir new civic  \n",
       "4   4    cadeirinha de carro bebê princesa princess  kgs"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste = pd.concat([num, test], axis=1)\n",
    "teste.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(train)\n",
    "del(test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos que convertar as strings para float, para poder alimentar o modelo. Iremos utilizar primeiro o BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustavo\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'até', 'com', 'como', 'con', 'contra', 'cual', 'cuando', 'da', 'das', 'de', 'del', 'dela', 'delas', 'dele', 'deles', 'depois', 'desde', 'do', 'donde', 'dos', 'durante', 'el', 'ela', 'elas', 'ele', 'eles', 'ella', 'ellas', 'ellos', 'em', 'en', 'entre', 'era', 'erais', 'eram', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'essa', 'essas', 'esse', 'esses', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estará', 'estarán', 'estarás', 'estaré', 'estaréis', 'estaría', 'estaríais', 'estaríamos', 'estarían', 'estarías', 'estas', 'estava', 'estavam', 'este', 'esteja', 'estejam', 'estejamos', 'estemos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéramos', 'estivéssemos', 'esto', 'estos', 'estou', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuviéramos', 'estuviésemos', 'estuvo', 'está', 'estábamos', 'estáis', 'están', 'estás', 'estávamos', 'estão', 'esté', 'estéis', 'estén', 'estés', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'forem', 'formos', 'fosse', 'fossem', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fuéramos', 'fuésemos', 'fôramos', 'fôssemos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habrá', 'habrán', 'habrás', 'habré', 'habréis', 'habría', 'habríais', 'habríamos', 'habrían', 'habrías', 'habéis', 'había', 'habíais', 'habíamos', 'habían', 'habías', 'haja', 'hajam', 'hajamos', 'han', 'has', 'hasta', 'havemos', 'havia', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hayáis', 'he', 'hei', 'hemos', 'houve', 'houvemos', 'houver', 'houvera', 'houveram', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houvermos', 'houverá', 'houverão', 'houveríamos', 'houvesse', 'houvessem', 'houvéramos', 'houvéssemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubiéramos', 'hubiésemos', 'hubo', 'há', 'hão', 'isso', 'isto', 'já', 'la', 'las', 'le', 'les', 'lhe', 'lhes', 'lo', 'los', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'mi', 'minha', 'minhas', 'mis', 'mucho', 'muchos', 'muito', 'muy', 'más', 'mí', 'mía', 'mías', 'mío', 'míos', 'na', 'nada', 'nas', 'nem', 'ni', 'no', 'nos', 'nosotras', 'nosotros', 'nossa', 'nossas', 'nosso', 'nossos', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'num', 'numa', 'não', 'nós', 'os', 'otra', 'otras', 'otro', 'otros', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'pero', 'poco', 'por', 'porque', 'qual', 'quando', 'que', 'quem', 'quien', 'quienes', 'qué', 'se', 'sea', 'seamos', 'sean', 'seas', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'serei', 'seremos', 'seria', 'seriam', 'será', 'serán', 'serás', 'serão', 'seré', 'seréis', 'sería', 'seríais', 'seríamos', 'serían', 'serías', 'seu', 'seus', 'seáis', 'sido', 'siendo', 'sin', 'sobre', 'sois', 'somos', 'son', 'sou', 'soy', 'su', 'sua', 'suas', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'são', 'sí', 'só', 'también', 'também', 'tanto', 'te', 'tem', 'temos', 'tendremos', 'tendrá', 'tendrán', 'tendrás', 'tendré', 'tendréis', 'tendría', 'tendríais', 'tendríamos', 'tendrían', 'tendrías', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'tengáis', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'tenéis', 'tenía', 'teníais', 'teníamos', 'tenían', 'tenías', 'ter', 'terei', 'teremos', 'teria', 'terá', 'terão', 'teríamos', 'teu', 'teus', 'teve', 'ti', 'tiene', 'tienen', 'tienes', 'tinha', 'tinham', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéramos', 'tivéssemos', 'todo', 'todos', 'tu', 'tua', 'tuas', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuviéramos', 'tuviésemos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 'tém', 'têm', 'tínhamos', 'tú', 'um', 'uma', 'un', 'una', 'uno', 'unos', 'você', 'vocês', 'vos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'ya', 'yo', 'às', 'él'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "stop_words = open('stopwords_pt_es.txt', encoding = 'utf-8').read().splitlines()\n",
    "\n",
    "bow = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2), stop_words = stop_words)\n",
    "bow_train = bow.fit_transform(treino['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfTransformer(use_idf=False).fit(bow_train)\n",
    "tf_train = tf.transform(bow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = 10, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustavo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Gustavo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:22:05\n"
     ]
    }
   ],
   "source": [
    "# train model on bag-of-words features\n",
    "start_time = time.time()\n",
    "lr.fit(tf_train, treino['category'])\n",
    "elapsed_time = int(time.time() - start_time)\n",
    "print('{:02d}:{:02d}:{:02d}'.format(elapsed_time // 3600, (elapsed_time % 3600 // 60), elapsed_time % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_test = bow.transform(teste['title'])\n",
    "tft = TfidfTransformer(use_idf=False).fit(bow_test)\n",
    "tf_test = tft.transform(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on validation set\n",
    "bow_test_preds = lr.predict(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('sample_submission.csv', header = 0, usecols = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['category'] = bow_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission_lrfinal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quiser saber se o computador roda tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices()) \n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
